{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"Womens Clothing E-Commerce Reviews.csv\")\n",
    "reviews.drop(columns=reviews.columns[0], axis=1, inplace=True)\n",
    "df = reviews.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed Review Text'] = df['Review Text'].fillna('').apply(lambda x: preprocessText(x, True))\n",
    "df['Processed Title'] = df['Title'].fillna('').apply(lambda x: preprocessText(x, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Key words/aspect extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lswht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download()# install the missing module\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(tokens):\n",
    "    tagged = pos_tag(tokens)\n",
    "    keywords = [word for word, tag in tagged if tag in ('NN', 'NNS', 'JJ')] # noun, singular or plural, adjective or numeral\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dress', 10929), ('size', 9355), ('fit', 9021), ('top', 8168), ('color', 6848), ('great', 6076), ('look', 4863), ('fabric', 4744), ('small', 4574), ('wear', 4305)]\n"
     ]
    }
   ],
   "source": [
    "# get key words from review text\n",
    "df['Keywords'] = df['Processed Review Text'].apply(extract_keywords)\n",
    "all_keywords = sum(df['Keywords'], [])\n",
    "keyword_freq = Counter(all_keywords)\n",
    "print(keyword_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('great', 1788), ('dress', 1649), ('cute', 1531), ('love', 1421), ('beautiful', 1404), ('top', 1172), ('perfect', 803), ('fit', 668), ('nice', 506), ('comfortable', 469)]\n"
     ]
    }
   ],
   "source": [
    "# get key words from review title\n",
    "df[\"Keywords_title\"] = df['Processed Title'].apply(extract_keywords)\n",
    "all_keywords_title = sum(df[\"Keywords_title\"], [])\n",
    "keyword_freq_title = Counter(all_keywords_title)\n",
    "print(keyword_freq_title.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The keywords extracted above seem to contain some common words in the corpus like great, look etc. \n",
    "Will try to adjust by using some Corpus library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\lswht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# add library from corpus for keywords extraction\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Calculate frequency distribution in the Brown Corpus\n",
    "brown_words = brown.words()\n",
    "fdist_brown = FreqDist(w.lower() for w in brown_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when threshold set to 0.001, still see words like great or love. adjusting the threshold to 0.0004\n",
    "def extract_keywords_withCorpus(tokens, fdist_external, threshold=0.0004): \n",
    "    # Filter out tokens that are too common in the external corpus\n",
    "    tagged = pos_tag(tokens)\n",
    "    filtered_tokens = [word for word, tag in tagged if tag in ('NN', 'NNS', 'JJ')]\n",
    "    keywords = [word for word in filtered_tokens if fdist_external.freq(word) < threshold]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Keywords_v2'] = df['Processed Review Text'].apply(lambda tokens: extract_keywords_withCorpus(tokens, fdist_brown))\n",
    "all_keywords_v2 = Counter(sum(df[\"Keywords_v2\"], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dress', 10929),\n",
       " ('size', 9355),\n",
       " ('fit', 9021),\n",
       " ('top', 8168),\n",
       " ('color', 6848),\n",
       " ('look', 4863),\n",
       " ('fabric', 4744),\n",
       " ('wear', 4305),\n",
       " ('love', 3969),\n",
       " ('soft', 3300),\n",
       " ('perfect', 3268),\n",
       " ('comfortable', 3007),\n",
       " ('beautiful', 2908),\n",
       " ('cute', 2887),\n",
       " ('nice', 2855)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_keywords_v2.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Keywords_title_v2'] = df['Processed Title'].apply(lambda tokens: extract_keywords_withCorpus(tokens, fdist_brown))\n",
    "all_keywords_title_v2 = Counter(sum(df[\"Keywords_title_v2\"], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dress', 1649),\n",
       " ('cute', 1531),\n",
       " ('love', 1421),\n",
       " ('beautiful', 1404),\n",
       " ('top', 1172),\n",
       " ('perfect', 803),\n",
       " ('fit', 668),\n",
       " ('nice', 506),\n",
       " ('comfortable', 469),\n",
       " ('color', 455),\n",
       " ('comfy', 453),\n",
       " ('gorgeous', 446),\n",
       " ('soft', 432),\n",
       " ('summer', 429),\n",
       " ('sweater', 420)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_keywords_title_v2.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dress', 12578),\n",
       " ('fit', 9689),\n",
       " ('size', 9581),\n",
       " ('top', 9340),\n",
       " ('color', 7303),\n",
       " ('love', 5390),\n",
       " ('look', 5041),\n",
       " ('fabric', 5026),\n",
       " ('cute', 4418),\n",
       " ('wear', 4407),\n",
       " ('beautiful', 4312),\n",
       " ('perfect', 4071),\n",
       " ('soft', 3732),\n",
       " ('comfortable', 3476),\n",
       " ('nice', 3361)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_counter = all_keywords_v2 + all_keywords_title_v2\n",
    "combined_counter.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Will filter the words manually later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are still words that seem to come from the same aspect. May worth to get the aspect categories from background dataset.\n",
    "# background data is a txt file with each row containing one review\n",
    "background_reviews = []\n",
    "with open('background_data_dev.txt') as f:\n",
    "    for line in f:\n",
    "        background_reviews.append(line.split('###')[0])\n",
    "\n",
    "df_background = pd.DataFrame(background_reviews, columns = [\"Review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('great', 39), ('good', 33), ('size', 31), ('shoe', 31), ('time', 28), ('year', 28), ('fit', 26), ('comfortable', 24), ('pair', 23), ('use', 18)]\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "df_background['Processed Review'] = df_background['Review'].fillna('').apply(lambda x: preprocessText(x, True))\n",
    "# get key words\n",
    "df_background['Keywords'] = df_background['Processed Review'].apply(extract_keywords)\n",
    "# Aggregating Keywords\n",
    "all_keywords_bg = sum(df_background['Keywords'], [])\n",
    "keyword_freq_bg = Counter(all_keywords_bg)\n",
    "\n",
    "# Display most common keywords\n",
    "print(keyword_freq_bg.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the key words do not seem to be substantially better than the main data. Will use the key words derived from original data review text + title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Key words/aspect extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp37-cp37m-win_amd64.whl (24.0 MB)\n",
      "     --------------------------------------- 24.0/24.0 MB 20.4 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.17.0 (from gensim)\n",
      "  Using cached numpy-1.21.6-cp37-cp37m-win_amd64.whl (14.0 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting Cython==0.29.28 (from gensim)\n",
      "  Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "     ------------------------------------- 983.8/983.8 kB 20.7 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 57.0/57.0 kB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: smart-open, numpy, Cython, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -qdm (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rllib3 (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\numpy-1.16.4.dist-info\\\\entry_points.txt'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\google\\auth\\__init__.py:55: Python37DeprecationWarning: After January 1, 2024, new releases of this library will drop support for Python 3.7. More details about Python 3.7 support can be found at https://cloud.google.com/python/docs/python37-sunset/\n",
      "  warnings.warn(message, Python37DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\google\\oauth2\\__init__.py:40: Python37DeprecationWarning: After January 1, 2024, new releases of this library will drop support for Python 3.7. More details about Python 3.7 support can be found at https://cloud.google.com/python/docs/python37-sunset/\n",
      "  warnings.warn(message, Python37DeprecationWarning)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lswht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Combined_Text'] = df['Review Text'].fillna('') + ' ' + df['Title'].fillna('')\n",
    "\n",
    "# Tokenize and remove stop words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "texts = [\n",
    "    [word for word in tokenizer.tokenize(document.lower()) if word not in stop_words]\n",
    "    for document in df['Combined_Text']\n",
    "]\n",
    "\n",
    "# Create a dictionary and corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.031*\"blue\" + 0.030*\"color\" + 0.021*\"white\" + 0.021*\"one\"')\n",
      "(1, '0.039*\"great\" + 0.034*\"sweater\" + 0.029*\"love\" + 0.026*\"soft\"')\n",
      "(2, '0.158*\"dress\" + 0.026*\"beautiful\" + 0.020*\"love\" + 0.014*\"flattering\"')\n",
      "(3, '0.020*\"size\" + 0.019*\"fit\" + 0.018*\"would\" + 0.014*\"like\"')\n",
      "(4, '0.071*\"top\" + 0.022*\"bra\" + 0.022*\"wear\" + 0.021*\"tank\"')\n",
      "(5, '0.084*\"5\" + 0.033*\"size\" + 0.028*\"fit\" + 0.027*\"length\"')\n",
      "(6, '0.042*\"size\" + 0.035*\"small\" + 0.026*\"large\" + 0.022*\"medium\"')\n",
      "(7, '0.033*\"like\" + 0.022*\"fabric\" + 0.018*\"top\" + 0.016*\"look\"')\n",
      "(8, '0.073*\"skirt\" + 0.053*\"color\" + 0.036*\"blouse\" + 0.036*\"beautiful\"')\n",
      "(9, '0.040*\"love\" + 0.030*\"great\" + 0.027*\"jeans\" + 0.022*\"wear\"')\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "num_topics = 10  # Adjust this based on your needs\n",
    "passes = 10  # Number of passes through corpus during training\n",
    "\n",
    "# Create and train the LDA model\n",
    "lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "By examing the data, and the aspects above, we came up with the aspects below.\n",
    "\n",
    "**list of words**:\n",
    "\n",
    "aspects = [\"dress\", \"love\", \"fit\", \"size\", \"top\", \"color\", \"look\", \"wear\", \"fabric\", \"cute\", \"flattering\", \"comfortable\"]\n",
    "\n",
    "**word cluster**:\n",
    "\n",
    "{\"name\": \"praise\", \"keywords\": [\"great\", \"love\", \"perfect\", \"wonderful\", \"good\", \"glad\", \"compliment\", \"favorite\", \"well\"]},\n",
    "\n",
    "{\"name\": \"fabric\", \"keywords\": [\"fabric\", \"soft\", \"silky\"]},\n",
    "\n",
    "{\"name\": \"size\", \"keywords\": [\"size\", \"small\", \"large\", \"fit\", \"length\"]},\n",
    "\n",
    "{\"name\": \"look\", \"keywords\": [\"look\", \"beautiful\", \"flattering\", \"sexy\", \"pretty\", \"flirty\", \"fabulous\"]},\n",
    "\n",
    "{\"name\": \"color\", \"keywords\": [\"color\", \"blue\", \"white\", \"black\"]},\n",
    "\n",
    "{\"name\": \"price\", \"keywords\": [\"price\", \"worth\", \"quality\"]}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
